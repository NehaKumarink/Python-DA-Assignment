{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcEYSIekqKDqJui9HGlcAK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NehaKumarink/Python-DA-Assignment/blob/main/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1) What is a parameter?**"
      ],
      "metadata": {
        "id": "hW1SZovNynjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Machine Learning (ML), a parameter is a variable that the model learns from the training data.\n",
        "\n",
        "These parameters define how the model makes predictions and are updated during the training process.\n",
        "\n",
        "**Examples of Parameters in ML:**\n",
        "\n",
        "- Weights (W) & Biases (b) in neural networks and linear regression models.\n",
        "\n",
        "- Coefficients in logistic regression.\n",
        "\n",
        "- Decision boundaries in decision trees."
      ],
      "metadata": {
        "id": "WJbWotP7ytMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2) What is correlation?**"
      ],
      "metadata": {
        "id": "-BT4dddkzFhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation in Machine Learning\n",
        "\n",
        "Correlation measures the relationship between two variablesâ€”how one variable changes in relation to another.\n",
        "\n",
        "It helps determine whether an increase in one feature leads to an increase or decrease in another.\n",
        "\n",
        "**Negative Correlation Mean**\n",
        "\n",
        "A negative correlation means that when one variable increases, the other decreases.\n",
        "\n",
        "The closer the correlation value is to -1, the stronger the inverse relationship."
      ],
      "metadata": {
        "id": "-OtJISZLzYO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3) Define Machine Learning. What are the main components in Machine Learning?**"
      ],
      "metadata": {
        "id": "qFMci2igzkS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning (ML) is a branch of artificial intelligence (AI) that enables computers to learn from data and make predictions or decisions without being explicitly programmed.\n",
        "\n",
        "It identifies patterns and relationships in data to improve performance over time.\n",
        "\n",
        "**Example**: A spam filter learns from past emails to classify new emails as spam or not."
      ],
      "metadata": {
        "id": "yOwD18nP0RXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4) How does loss value help in determining whether the model is good or not**?"
      ],
      "metadata": {
        "id": "cd4skiYM0o25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss value is a numerical representation of how far the model's predictions are from the actual values.\n",
        "\n",
        "It helps in determining whether the model is good or needs improvement.\n",
        "\n",
        "A lower loss means better performance, while a higher loss indicates poor predictions.\n",
        "\n",
        "A good model is one that generalizes well to unseen data, meaning it performs well not just on the training set but also on the test/validation set.\n",
        "\n",
        "You can determine this using loss values, evaluation metrics, and visual analysis."
      ],
      "metadata": {
        "id": "-TSJ55tE05cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Actual vs. Predicted labels for classification\n",
        "y_true = [0, 1, 1, 0, 1, 0, 1]\n",
        "y_pred = [0, 1, 0, 0, 1, 0, 1]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma1_zWhI1pgJ",
        "outputId": "d3a4e1c6-186f-48fe-e227-6b408ecfcbb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5) What are continuous and categorical variables?**"
      ],
      "metadata": {
        "id": "vTcOmrKL12mN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Continuous Variables** (Numeric Data)\n",
        "\n",
        "Definition: Variables that can take an infinite number of values within a range.\n",
        "\n",
        "Examples:\n",
        "Height (e.g., 5.6 feet, 6.1 feet)\n",
        "Weight (e.g., 65.5 kg, 72.3 kg)\n",
        "Temperature (e.g., 36.5Â°C, 40.2Â°C)\n",
        "\n",
        "Characteristics:\n",
        "Can have decimal values (e.g., 5.75)\n",
        "Measured, not counted\n",
        "Often used in regression models\n",
        "\n",
        "**Categorical Variables**\n",
        "\n",
        "Definition: Variables that represent distinct groups or categories.\n",
        "\n",
        "Examples:\n",
        "Binary: Yes/No, Male/Female\n",
        "Nominal (No order): Colors (Red, Blue, Green), Car Brands (Toyota, BMW)\n",
        "Ordinal (Has order): Education Level (High School, Bachelorâ€™s, Masterâ€™s)\n",
        "\n",
        "**Characteristics**:\n",
        "\n",
        "Represent categories or groups\n",
        "\n",
        "Can be nominal (no order) or ordinal (ordered categories)\n",
        "\n",
        "Used in classification models\n"
      ],
      "metadata": {
        "id": "HaOLhHuu2Ylr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6) How do we handle categorical variables in Machine Learning? What are the common techniques?**"
      ],
      "metadata": {
        "id": "zYS4Whyc4Av4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling categorical variables in ML:\n",
        "\n",
        "One-Hot Encoding â€“ Creates binary columns for each category (best for low-cardinality nominal data).\n",
        "\n",
        "Label Encoding â€“ Assigns unique integers to categories (use for ordinal data).\n",
        "\n",
        "Ordinal Encoding â€“ Respects category order (e.g., \"Low\" < \"Medium\" < \"High\").\n",
        "\n",
        "Target Encoding â€“ Replaces categories with target mean (risk of data leakage).\n",
        "\n",
        "Frequency Encoding â€“ Uses category occurrence count.\n",
        "\n",
        "Hash Encoding â€“ Converts categories into fixed-length numerical hashes (useful for high-cardinality data)."
      ],
      "metadata": {
        "id": "3RzaZHtR5HCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7)What do you mean by training and testing a dataset?**"
      ],
      "metadata": {
        "id": "Wocv67z05ZYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Machine Learning, training and testing a dataset refers to splitting data to evaluate model performance.\n",
        "\n",
        "Training Dataset: Used to train the model by learning patterns and relationships.\n",
        "\n",
        "Testing Dataset: Used to assess the modelâ€™s accuracy and generalization on unseen data.\n",
        "\n",
        "Typically, data is split 80% for training and 20% for testing, but ratios can vary. This helps prevent overfitting and ensures the model performs well on new data."
      ],
      "metadata": {
        "id": "bs39v0p951iq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8) What is sklearn.preprocessing?**"
      ],
      "metadata": {
        "id": "V1PpC5Yx55zS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a module in Scikit-Learn that provides tools for transforming and normalizing data before training ML models.\n",
        "\n",
        "It helps improve model performance by scaling, encoding, and modifying features."
      ],
      "metadata": {
        "id": "pxTPBjnB6AJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9) What is a Test set?**"
      ],
      "metadata": {
        "id": "HjwFSVsJ6r0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, a test set is a dataset used to evaluate the performance of a trained model.\n",
        "\n",
        "It consists of data that the model has never seen before during training, allowing for an unbiased assessment of how well the model generalizes to new data.\n",
        "\n",
        "Key Points About the Test Set:\n",
        "- Used for final evaluation â€“ It helps determine the real-world effectiveness of the model.\n",
        "- Separate from training & validation sets â€“ Prevents data leakage and overfitting.\n",
        "- Performance metrics â€“ Common evaluation metrics include accuracy, precision, recall, F1-score, and RMSE.\n",
        "\n",
        "Example Usage\n",
        "\n",
        "Training Set â€“ Used to train the model.\n",
        "\n",
        "Validation Set â€“ Used to fine-tune hyperparameters.\n",
        "\n",
        "Test Set â€“ Used to evaluate the final modelâ€™s accuracy and generalization ability."
      ],
      "metadata": {
        "id": "yrrQnbCRdsYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10)How do we split data for model fitting (training and testing) in Python?** **How to Approach a Machine Learning Problem?**"
      ],
      "metadata": {
        "id": "D0TAOu_Pd6WB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, we typically use scikit-learn's train_test_split() function to split data into training and test sets."
      ],
      "metadata": {
        "id": "aTbSAS_deM4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample dataset (features X, target y)\n",
        "X = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
        "y = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
        "\n",
        "# Splitting data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set size:\", len(X_train))\n",
        "print(\"Test set size:\", len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbHLpxTyeVt5",
        "outputId": "f64b6c7a-dc81-4309-9b73-d82d4cdb0abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 8\n",
            "Test set size: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A structured approach ensures efficiency and accuracy.\n",
        "\n",
        "Step-by-Step Approach:\n",
        "Step 1: Define the Problem\n",
        "\n",
        "Understand the business problem.\n",
        "\n",
        "Identify the type of ML problem (Classification, Regression, Clustering, etc.).\n",
        "\n",
        "Step 2: Collect and Explore Data\n",
        "\n",
        "Gather relevant data (CSV, databases, APIs).\n",
        "Perform Exploratory Data Analysis (EDA): check for missing values, outliers, distributions.\n",
        "\n",
        "Step 3: Preprocess the Data\n",
        "\n",
        "Handle missing values, duplicates.\n",
        "Encode categorical variables.\n",
        "Normalize/Standardize numerical features if needed.\n",
        "\n",
        "Step 4: Split Data\n",
        "\n",
        "Use train_test_split() to create training and testing sets.\n",
        "Sometimes, create a validation set for hyperparameter tuning.\n",
        "\n",
        "Step 5: Choose a Model\n",
        "\n",
        "Select a suitable algorithm (e.g., Decision Tree, SVM, Neural Networks).\n",
        "\n",
        "Step 6: Train the Model\n",
        "\n",
        "Fit the model using the training data.\n",
        "Tune hyperparameters for better performance.\n",
        "\n",
        "Step 7: Evaluate the Model\n",
        "\n",
        "Use metrics like accuracy, precision, recall, RMSE, RÂ² score to measure performance.\n",
        "Check for overfitting/underfitting.\n",
        "\n",
        "Step 8: Improve the Model\n",
        "\n",
        "Try different feature engineering techniques.\n",
        "Tune hyperparameters using GridSearchCV or RandomizedSearchCV.\n",
        "Experiment with different models (e.g., Ensemble Learning).\n",
        "\n",
        "Step 9: Deploy the Model\n",
        "\n",
        "Save the model using joblib or pickle.\n",
        "Deploy it as an API using Flask, FastAPI, or Django.\n",
        "\n",
        "Step 10: Monitor & Maintain\n",
        "\n",
        "Track performance on new data.\n",
        "\n",
        "Update the model as data evolves."
      ],
      "metadata": {
        "id": "g1azYczceirc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11) Why do we have to perform EDA before fitting a model to the data?**"
      ],
      "metadata": {
        "id": "td4mleN1fWSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA is a crucial step in Machine Learning because it helps you understand, clean, and prepare your data before fitting it into a model.\n",
        "\n",
        "Hereâ€™s why itâ€™s essential:\n",
        "\n",
        "1ï¸ Understand the Data Structure\n",
        "Identifies features (columns) and target variable.\n",
        "Understands data types (numerical, categorical, text).\n",
        "Helps decide which features to use in the model.\n",
        "\n",
        "2ï¸ Detect & Handle Missing Values\n",
        "Missing data can cause errors or bias in the model.\n",
        "Common techniques: removal, imputation (mean, median, mode), or using algorithms that handle missing data.\n",
        "\n",
        "3ï¸ Identify Outliers\n",
        "Outliers can skew predictions and cause poor model performance.\n",
        "Visualization tools like box plots, scatter plots help detect them.\n",
        "Outliers can be removed or treated using transformations.\n",
        "\n",
        "4ï¸ Detect Data Imbalance (for Classification)\n",
        "Imbalanced datasets (e.g., 95% Class A, 5% Class B) lead to biased models.\n",
        "Solution: Use oversampling (SMOTE), undersampling, or weighted loss functions.\n",
        "\n",
        "5ï¸ Check Feature Correlations\n",
        "Identifies highly correlated features, which may lead to multicollinearity.\n",
        "Feature selection techniques (e.g., Variance Inflation Factor (VIF)) help remove redundant features.\n",
        "\n",
        "6 Choose the Right Data Transformations\n",
        "Normalization & Standardization for numerical features (especially for distance-based models like KNN, SVM).\n",
        "Encoding categorical variables (One-Hot Encoding, Label Encoding).\n",
        "\n",
        "7 Select the Right Model & Feature Engineering\n",
        "EDA guides feature selection, engineering, and model choice.\n",
        "Example: If features are highly correlated, Decision Trees may perform better than Linear Regression.\n"
      ],
      "metadata": {
        "id": "TdtXLcBYf7bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12)What is correlation?"
      ],
      "metadata": {
        "id": "Rr-uBC7YghGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that describes the relationship between two variables. It shows whether and how strongly they move together.\n",
        "\n",
        "Positive correlation: Both variables increase or decrease together.\n",
        "\n",
        "Negative correlation: One variable increases while the other decreases.\n",
        "\n",
        "No correlation: No clear relationship between the variables.\n",
        "\n",
        "Itâ€™s usually measured using the correlation coefficient (r), ranging from -1 (strong negative) to +1 (strong positive), with 0 meaning no correlation."
      ],
      "metadata": {
        "id": "rRdfmwmUWWM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13) What does negative correlation mean?"
      ],
      "metadata": {
        "id": "v-bi7B4OWbxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative correlation means that when one variable increases, the other decreases. They move in opposite directions.\n",
        "\n",
        "For example:\n",
        "\n",
        "The more you exercise, the less you weigh.\n",
        "\n",
        "The more time spent on social media, the lower the grades (possibly)."
      ],
      "metadata": {
        "id": "BV6FvDWPWpf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14) How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "N29LChoqWr4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find the correlation between variables in Python using Pandas and NumPy. Here are some common methods:\n",
        "\n",
        "Using corr() in Pandas (for DataFrames)"
      ],
      "metadata": {
        "id": "mGTitYLJW2ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {'A': [1, 2, 3, 4, 5], 'B': [5, 4, 3, 2, 1]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compute correlation\n",
        "correlation = df.corr()\n",
        "print(correlation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI2B1wQJW7l6",
        "outputId": "88ed7255-4f4d-4de5-9dec-1cd8a55ac65f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     A    B\n",
            "A  1.0 -1.0\n",
            "B -1.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([5, 4, 3, 2, 1])\n",
        "\n",
        "correlation_matrix = np.corrcoef(x, y)\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0Z2THtbXEbC",
        "outputId": "fccc8830-8631-48e1-dc89-55eed538b0ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1. -1.]\n",
            " [-1.  1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [5, 4, 3, 2, 1]\n",
        "\n",
        "corr, _ = pearsonr(x, y)\n",
        "print(f'Pearson correlation: {corr}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et_MSpSJXG3C",
        "outputId": "3ccc25fa-dd44-40a7-ead0-2a8361246411"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation: -1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15) What is causation? Explain difference between correlation and causation with an example.\n"
      ],
      "metadata": {
        "id": "wiqWoq4zXRLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causation means that one event directly causes another. If A causes B, changing A will directly result in a change in B.\n",
        "\n",
        "Correlation vs. Causation\n",
        "\n",
        "Correlation: Two variables are related but one does not necessarily cause the other.\n",
        "\n",
        "Causation: One variable directly influences the other.\n",
        "\n",
        "Example:\n",
        "Correlation: Ice cream sales and drowning incidents increase in summer.\n",
        "\n",
        "More ice cream does not cause drowning! They are related due to hot weather (a third factor).\n",
        "\n",
        "Causation: Drinking alcohol and impaired driving ability.\n",
        "More alcohol directly reduces driving ability."
      ],
      "metadata": {
        "id": "9_nQCh7UXcTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16) What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "JeHEIgcOXnyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An optimizer is an algorithm used in machine learning and deep learning to adjust the parameters (weights and biases) of a model to minimize the loss function and improve performance.\n",
        "\n",
        "It helps the model learn by updating weights efficiently to achieve better predictions.\n",
        "\n",
        "Types of Optimizers\n",
        "\n",
        "Optimizers can be broadly classified into two categories:\n",
        "\n",
        "First-order optimizers (use gradients of the loss function)\n",
        "\n",
        "Second-order optimizers (use second derivatives like Hessian matrices, which are computationally expensive)\n",
        "\n",
        "The most commonly used optimizers are first-order optimizers based on Gradient Descent.\n",
        "\n",
        "1. Gradient Descent (GD)\n",
        "This is the simplest optimization algorithm that minimizes the loss function by iteratively updating the weights in the opposite direction of the gradient.\n",
        "\n",
        "Types of Gradient Descent:\n",
        "Batch Gradient Descent:\n",
        "\n",
        "Computes gradient using the entire dataset.\n",
        "Pros: Converges smoothly.\n",
        "Cons: Slow for large datasets.\n",
        "Example: Training a simple linear regression model on a small dataset.\n",
        "Stochastic Gradient Descent (SGD):\n",
        "\n",
        "Updates weights for each individual data point.\n",
        "Pros: Faster, works well for large datasets.\n",
        "Cons: High variance in updates, can be noisy.\n",
        "Example: Training an image classification model on millions of images.\n",
        "Mini-batch Gradient Descent:\n",
        "\n",
        "Uses small batches of data instead of the whole dataset or a single data point.\n",
        "Pros: Balances speed and stability.\n",
        "Example: Training a deep learning model using batches of 32 or 64 samples.\n",
        "\n",
        "2. Momentum-based Optimizer\n",
        "Momentum helps the optimizer move faster by accumulating past gradients, reducing oscillations in the updates.\n",
        "\n",
        "Example:\n",
        "Used in deep learning models to speed up convergence in CNNs and RNNs.\n",
        "\n",
        "3. Adaptive Learning Rate Optimizers\n",
        "These adjust the learning rate dynamically during training.\n",
        "\n",
        "(i) AdaGrad (Adaptive Gradient Algorithm)\n",
        "Gives larger updates for infrequent parameters and smaller updates for frequent ones.\n",
        "Pros: Good for sparse data (e.g., NLP).\n",
        "Cons: Learning rate keeps decreasing over time.\n",
        "Example:\n",
        "Used in text-based applications like word embeddings.\n",
        "\n",
        "(ii) RMSprop (Root Mean Square Propagation)\n",
        "Solves AdaGrad's problem by maintaining an exponentially decaying average of squared gradients.\n",
        "Pros: Works well for RNNs.\n",
        "Cons: Requires careful tuning.\n",
        "Example:\n",
        "Used in speech recognition and NLP models.\n",
        "\n",
        "(iii) Adam (Adaptive Moment Estimation)\n",
        "Combines Momentum and RMSprop, making it one of the most widely used optimizers.\n",
        "Pros: Works well for most deep learning problems.\n",
        "Cons: Can sometimes generalize poorly.\n",
        "Example:\n",
        "Used in training deep neural networks for tasks like image recognition and NLP.\n",
        "\n",
        "4. AdamW (Adam with Weight Decay)\n",
        "An improved version of Adam that fixes weight decay handling.\n",
        "Pros: Better generalization.\n",
        "Example: Used in transformers like BERT for NLP tasks.\n"
      ],
      "metadata": {
        "id": "ev9Gz_cYYTYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17) What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "i-xOGTXwZLxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "klearn.linear_model is a module in Scikit-Learn that provides various linear models for regression and classification tasks.\n",
        "\n",
        "It includes popular algorithms like Linear Regression, Logistic Regression, Ridge, Lasso, ElasticNet, SGD (Stochastic Gradient Descent), and more."
      ],
      "metadata": {
        "id": "tYeAOCT9ZfxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()"
      ],
      "metadata": {
        "id": "KecWGxwkZ4eu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()"
      ],
      "metadata": {
        "id": "b_3PSXEFaEI1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "ridge = Ridge(alpha=1.0)\n",
        "lasso = Lasso(alpha=0.1)"
      ],
      "metadata": {
        "id": "-q55K_MxaIsf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)"
      ],
      "metadata": {
        "id": "AogjXSTIaMRm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
        "sgd_reg = SGDRegressor()\n",
        "sgd_clf = SGDClassifier()"
      ],
      "metadata": {
        "id": "zoa_wdiyaRxp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Use sklearn.linear_model?\n",
        "\n",
        "When working with structured/tabular data.\n",
        "\n",
        "When assuming a linear relationship between input features and target.\n",
        "\n",
        "When using regularization to prevent overfitting.\n",
        "\n",
        "When dealing with large datasets (SGD works well with online learning)."
      ],
      "metadata": {
        "id": "7KNsVMdaabHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18) What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "jIankJwLakx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, model.fit() is a method used to train a model using provided data. It adjusts the modelâ€™s parameters based on input data and corresponding target labels.\n",
        "\n",
        "Functionality of model.fit()\n",
        "When called, model.fit():\n",
        "\n",
        "Feeds the input data to the model\n",
        "Performs forward and backward passes (calculates loss and updates model weights using backpropagation)\n",
        "Iterates over multiple epochs (passes over the entire dataset multiple times)\n",
        "Monitors training metrics (e.g., loss, accuracy)\n",
        "Arguments of model.fit()\n",
        "Depending on the library (e.g., TensorFlow/Keras, PyTorch), the required arguments may vary. Hereâ€™s how it works in Keras (TensorFlow):\n",
        "\n",
        "Required Arguments\n",
        "x: Input training data (numpy array, tensor, or dataset)\n",
        "y: Target labels (for supervised learning)\n",
        "Common Optional Arguments\n",
        "epochs: Number of times the model will iterate over the dataset\n",
        "batch_size: Number of samples per gradient update\n",
        "validation_data: Data used for validation (tuple of (x_val, y_val))\n",
        "shuffle: Whether to shuffle data before each epoch (True by default)\n",
        "callbacks: List of functions to monitor and modify training (e.g., EarlyStopping)\n",
        "verbose: Logging level (0 for silent, 1 for progress bar, 2 for one log per epoch)"
      ],
      "metadata": {
        "id": "AGIdzwU1a2h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Dummy dataset\n",
        "x_train = np.random.rand(1000, 10)\n",
        "y_train = np.random.randint(0, 2, size=(1000,))\n",
        "\n",
        "# Define a simple model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(32, activation='relu', input_shape=(10,)),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEkAo0aRa5_c",
        "outputId": "e12faf16-2545-46e8-9aa5-18235a319011"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.4880 - loss: 0.7006 - val_accuracy: 0.5200 - val_loss: 0.6829\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5017 - loss: 0.7039 - val_accuracy: 0.5050 - val_loss: 0.6837\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5367 - loss: 0.6948 - val_accuracy: 0.5400 - val_loss: 0.6839\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5409 - loss: 0.6910 - val_accuracy: 0.5650 - val_loss: 0.6840\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5544 - loss: 0.6878 - val_accuracy: 0.5800 - val_loss: 0.6841\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5579 - loss: 0.6844 - val_accuracy: 0.5700 - val_loss: 0.6841\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5961 - loss: 0.6744 - val_accuracy: 0.5750 - val_loss: 0.6847\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5747 - loss: 0.6785 - val_accuracy: 0.5400 - val_loss: 0.6871\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5740 - loss: 0.6785 - val_accuracy: 0.5600 - val_loss: 0.6854\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5442 - loss: 0.6858 - val_accuracy: 0.5400 - val_loss: 0.6883\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ff2e293d910>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19) What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "cqWoJ0NJbBBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.predict() Overview\n",
        "The model.predict() method is used to generate predictions from a trained machine learning model. It takes input data and returns the model's output, typically probabilities or class labels (depending on the model type).\n",
        "\n",
        "Functionality of model.predict()\n",
        "Takes input data (without labels, since we're making predictions).\n",
        "Performs a forward pass through the model.\n",
        "Outputs predictions (e.g., probabilities, regression values, or class labels).\n",
        "Arguments of model.predict()\n",
        "In TensorFlow/Keras, the most commonly used arguments are:\n",
        "\n",
        "Required\n",
        "x: The input data (NumPy array, TensorFlow tensor, or dataset).\n",
        "Optional\n",
        "batch_size: Number of samples per batch for computation (default: automatic selection).\n",
        "verbose: Logging level (0 = silent, 1 = progress bar).\n",
        "steps: Number of batches to process (for generators).\n",
        "callbacks: Custom functions to monitor prediction.\n"
      ],
      "metadata": {
        "id": "arZFUH0tbLXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dummy test data\n",
        "x_test = np.random.rand(5, 10)\n",
        "\n",
        "# Generate predictions\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "# Print results\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzWUgspzbQDL",
        "outputId": "dfb95e40-613c-4ab3-b974-0c92ef1785a0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "[[0.42121094]\n",
            " [0.42983934]\n",
            " [0.5119582 ]\n",
            " [0.5330314 ]\n",
            " [0.47106582]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your model is a classification model, predictions will contain probabilities (e.g., for binary classification, values between 0 and 1).\n",
        "\n",
        "For binary classification, you may convert probabilities to class labels like this:"
      ],
      "metadata": {
        "id": "gPDNmxoibhOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = (predictions > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "GhRtph7sbkeb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20) What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "y59UqOIpbTxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous vs. Categorical Variables\n",
        "In data science and statistics, variables are classified based on the type of data they represent. The two main types are continuous and categorical variables.\n",
        "\n",
        "1. Continuous Variables\n",
        "A continuous variable can take an infinite number of values within a given range. These variables are measurable and often represent quantities.\n",
        "\n",
        "Examples:\n",
        "Height (e.g., 170.5 cm, 172.2 cm)\n",
        "Weight (e.g., 65.3 kg, 72.8 kg)\n",
        "Temperature (e.g., 36.6Â°C, 98.4Â°F)\n",
        "Salary (e.g., $45,500.75, $60,100.20)\n",
        "Key Characteristics:\n",
        "âœ” Can take decimal or fractional values\n",
        "âœ” Can be measured precisely\n",
        "âœ” Can be transformed (e.g., normalized, standardized)\n",
        "\n",
        "2. Categorical Variables\n",
        "A categorical variable represents a finite number of distinct groups or categories. These variables are not measurable but can be counted or labeled.\n",
        "\n",
        "Types of Categorical Variables:\n",
        "Nominal: Categories have no inherent order\n",
        "\n",
        "Examples:\n",
        "Gender (Male, Female, Other)\n",
        "Blood Type (A, B, AB, O)\n",
        "Eye Color (Blue, Green, Brown)\n",
        "Ordinal: Categories have a meaningful order but no fixed numerical difference\n",
        "\n",
        "Examples:\n",
        "Education Level (High School, Bachelor's, Master's, PhD)\n",
        "Customer Satisfaction (Low, Medium, High)\n",
        "Economic Status (Low Income, Middle Income, High Income)\n",
        "Key Characteristics:\n",
        "âœ” Represent distinct groups\n",
        "âœ” Cannot take fractional values\n",
        "âœ” Can be encoded as numbers (e.g., One-Hot Encoding, Label Encoding)"
      ],
      "metadata": {
        "id": "PCzP4lQtbmjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21) What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "MVebvqrjbxNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling ensures all numerical features have a similar range, improving model performance and training speed.\n",
        "\n",
        "ðŸ”¹ Why Use It?\n",
        "\n",
        "Prevents large values from dominating smaller ones\n",
        "Speeds up training (especially for Gradient Descent)\n",
        "Improves accuracy in distance-based models (KNN, SVM, K-Means)\n",
        "\n",
        "ðŸ”¹ Common Methods:\n",
        "\n",
        "Standardization (Z-score) â€“ Centers data around 0, best for SVM, Linear Regression.\n",
        "\n",
        "Min-Max Scaling â€“ Scales between 0 and 1, best for Neural Networks, KNN.\n",
        "\n",
        "Robust Scaling â€“ Handles outliers, uses median & IQR.\n",
        "\n",
        "ðŸ”¹ Needed for: KNN, SVM, Neural Networks\n",
        "ðŸ”¹ Not needed for: Decision Trees, Random Forest"
      ],
      "metadata": {
        "id": "oTOfK6mAcV28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22) How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "jxZM-tL7cinP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling in Python is done using scikit-learn's MinMaxScaler or StandardScaler:\n",
        "\n",
        "MinMax Scaling (Normalization): Scales data to a fixed range (e.g., 0 to 1).\n",
        "\n",
        "Standard Scaling (Z-score normalization): Centers data with mean 0 and standard deviation 1."
      ],
      "metadata": {
        "id": "oZt6FzpvcxmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23) What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "fjj8JGKcc-na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a module in scikit-learn that provides tools for scaling, normalizing, encoding, and transforming data before training machine learning models.\n",
        "\n",
        "Common functions:\n",
        "\n",
        "Scaling: StandardScaler, MinMaxScaler\n",
        "Normalization: Normalizer\n",
        "Encoding: LabelEncoder, OneHotEncoder\n",
        "Imputation: SimpleImputer"
      ],
      "metadata": {
        "id": "61_TwHFKdUvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24) How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "Ywd2LwEMdXWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, we split data into training and testing sets using train_test_split from scikit-learn:"
      ],
      "metadata": {
        "id": "jbAl9o-edmkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'Age': [25, 30, 35, 40, 45, 50, 55, 60, 65, 70],\n",
        "    'Salary': [3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000],\n",
        "    'Purchased': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]  # Target variable (Binary classification)\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Features (X) and Target (y)\n",
        "X = df[['Age', 'Salary']]  # Independent variables\n",
        "y = df['Purchased']         # Dependent variable\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the shapes of the resulting datasets\n",
        "print(\"Training set size:\", X_train.shape, y_train.shape)\n",
        "print(\"Testing set size:\", X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_MEcBM0d4AB",
        "outputId": "50e6aeaa-71ee-4bee-bc36-b320f17808be"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (8, 2) (8,)\n",
            "Testing set size: (2, 2) (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25) Explain data encoding?"
      ],
      "metadata": {
        "id": "E4_9huaYeETM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding is the process of converting categorical data into numerical format so that machine learning models can process it.\n",
        "\n",
        "Types of Data Encoding:\n",
        "\n",
        "1. Label Encoding (Ordinal Encoding)\n",
        "Converts categorical labels into numbers (0,1,2,...).\n",
        "Works when categories have a meaningful order."
      ],
      "metadata": {
        "id": "v_5IMl_neN7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = ['Low', 'Medium', 'High']\n",
        "encoder = LabelEncoder()\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "print(encoded_data)  # Output: [1, 2, 0] (order may vary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCmKs2QHeU3c",
        "outputId": "823c92c0-05f7-45b2-d73e-9b29c73723c7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. One-Hot Encoding\n",
        "\n",
        "Converts categorical values into binary (0s and 1s) columns.\n",
        "Suitable for nominal data (no order)."
      ],
      "metadata": {
        "id": "1DqPOetneWym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "encoded_data = encoder.fit_transform(df)\n",
        "print(encoded_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKCLRTENecY-",
        "outputId": "c5524a8f-51b7-4d84-855a-dfa01e5574eb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Ordinal Encoding\n",
        "Assigns numbers based on a predefined order (e.g., Low < Medium < High).\n"
      ],
      "metadata": {
        "id": "1KJ-syD_ehzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "data = [['Low'], ['Medium'], ['High']]\n",
        "encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "print(encoded_data)  # Output: [[0.], [1.], [2.]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9Zs70hreqFU",
        "outputId": "51d29feb-d53d-4336-c4f6-e1023f93c9ad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.]\n",
            " [1.]\n",
            " [2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Frequency Encoding\n",
        "Replaces categories with their occurrence count in the dataset."
      ],
      "metadata": {
        "id": "6xs-lv8Re06i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'City': ['Dubai', 'Abu Dhabi', 'Dubai', 'Sharjah', 'Dubai']})\n",
        "freq_encoding = df['City'].value_counts().to_dict()\n",
        "df['City_Encoded'] = df['City'].map(freq_encoding)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM_SAL98e1rP",
        "outputId": "e83e8bdb-2181-4b3b-a658-ca0bdbdb7c62"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        City  City_Encoded\n",
            "0      Dubai             3\n",
            "1  Abu Dhabi             1\n",
            "2      Dubai             3\n",
            "3    Sharjah             1\n",
            "4      Dubai             3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Target Encoding (for categorical target variables)\n",
        "Replaces categories with their mean target value in classification problems."
      ],
      "metadata": {
        "id": "Vn9xr3__e8Ek"
      }
    }
  ]
}